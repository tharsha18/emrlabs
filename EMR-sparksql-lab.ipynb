{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Notebook using Spark Sql <a name=\"top\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we will do the following activities:\n",
    "    \n",
    "- Use a data set from S3 in CSV format and load it into Spark, create a spark data frame, create temporary table, query using spark sql\n",
    "- Use a pre-cataloged dataset in S3 using AWS Glue crawler and transform tables using spark sql\n",
    "- Write the transformed output to S3 in parquet format, crawl it to crate a catalog entryu, query using spark sql.\n",
    "\n",
    "Let's start by connecting to our our AWS Glue Dev Endpoint - a persistent AWS Glue Spark  Development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:13:36.786585Z",
     "start_time": "2020-05-20T20:13:12.192109Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:24.960531Z",
     "start_time": "2020-05-20T20:14:21.561942Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use salesdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regular Spark SQL commands work great as we have enabled the feature 'Use Glue Data Catalog as the Hive metastore' for our AWS Glue Dev Endpoint by default. You can choose to run any spark-sql commands against these tables as an optional exercise \n",
    "\n",
    "You can click on the link to read more on [AWS Glue Data Catalog Support for Spark SQL Jobs](\n",
    "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-data-catalog-hive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above tables are pre-created for you using the AWS Glue crawler and you can see any new EMR cluster can seamlessly access the tables. Now, what about files that are in S3 (say CSV) which you need to use spark and create a table using a data frame and query it in sql? Use the section below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV files from S3 into spark program and create tables to query in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql import Row # A row of data in a DataFrame\n",
    "from pyspark.sql import GroupedData # Aggregation methods, returned by DataFrame.groupBy().\n",
    "from pyspark.sql import DataFrameNaFunctions # Methods for handling missing data (null values).\n",
    "from pyspark.sql import DataFrameStatFunctions # Methods for statistics functionality.\n",
    "from pyspark.sql import functions # List of built-in functions available for DataFrame.\n",
    "from pyspark.sql import types # List of data types available.\n",
    "from pyspark.sql import Window # For working with window functions.\n",
    "#End Boilerplate code\n",
    "\n",
    "nytrip_df = spark.read.csv(\"s3://glue-labs-001-180486424913/data/nyc_trips_csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print schema for the csv files read in previous step\n",
    "nytrip_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run sql commands on the above spark dataframe, we will create a temp table. This table will persist through the life of this spark session\n",
    "nytrip_df.createOrReplaceTempView(\"temp_nytripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate your glue metastore and see if this table shows as a temp table as per third column.\n",
    "spark.sql(\"use salesdb\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fire at will using your standard ANSI sql queries against the tables in the catalog\n",
    "spark.sql(\"select * from temp_nytripdata\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform your data by denormalizing the tables and writing them in parquet format\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this activity, we will denormalize two tables and create a Parquet format output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the dataset\n",
    "\n",
    "Let's now denormalize the source tables in Glue catalog and write out the transformed output in Parquet format to the destination location. Note to change the S3 output_path in the cells below to appropriate bucket in your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify sample data and schema\n",
    "adf=spark.sql(\"select * from product_category limit 5\")\n",
    "adf.printSchema()\n",
    "bdf=spark.sql(\"select * from product limit 5\")\n",
    "bdf.printSchema()\n",
    "#Verify count on source\n",
    "adf.count()\n",
    "bdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:09.244279Z",
     "start_time": "2020-05-20T20:32:59.877787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create product denorm table joining between product_category and product tables\n",
    "product_df=spark.sql(\"SELECT a.category_id,a.category_name, b.product_id,b.name,b.supplier_id \\\n",
    "FROM product_category a, product b \\\n",
    "WHERE a.category_id=b.category_id\")\n",
    "\n",
    "#Lets verify the schema and record count\n",
    "product_df.printSchema()\n",
    "print(product_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the entire product denorm table above into 1 file in parquet file format. Make Sure to update the S3 path below and replace it with your \n",
    "\n",
    "product_df.coalesce(1).write.mode(\"OVERWRITE\").parquet(\"s3://glue-labs-001-180486424913/data/sales_analytics/product_dim/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that your data is written to S3, switch to AWS console and validate you have 1 parquet file created at the above S3 location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step, Crawl the Transformed Data and create a table in your catalog for querying\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **sales_analytics_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/sales_analytics/**\n",
    "    - Page: Choose an IAM role\n",
    "        - IAM Role: Choose an existing IAM role **glue-labs-GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **sales_analytics**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets query your transformed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:32.257757Z",
     "start_time": "2020-05-20T20:35:31.387873Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate your table exisits and start querying\n",
    "spark.sql(\"use sales_analytics\").show()\n",
    "spark.sql(\"show tables\").show()\n",
    "spark.sql(\"select * from sales_analytics.replacethiswith your TableName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!!! You have now successfully completed this exercise and learned how to use spark in your day-to-day\n",
    "[(Back to the top)](#top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
