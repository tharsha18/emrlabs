{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation using AWS Glue Dev Endpoints and Sagemaker Notebook <a name=\"top\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we will do the following activities:\n",
    "    \n",
    "- Build a Star (Denormalized) Schema from an OLTP 3NF (3rd Normal Form) Schema.\n",
    "- Write the derived table for denorm data set in parquet format partitioned out by key fields.\n",
    "- Finally, orchestrate the pipeline to create an AWS Glue Workflow.\n",
    "\n",
    "Let's start by connecting to our our AWS Glue Dev Endpoint - a persistent AWS Glue Spark  Development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:13:36.786585Z",
     "start_time": "2020-05-20T20:13:12.192109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56cd0485b074a6a9eac8de85941b475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1597082089998_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-52-143.ec2.internal:20888/proxy/application_1597082089998_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-48-25.ec2.internal:8042/node/containerlogs/container_1597082089998_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2.4.5-amzn-0'"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:24.960531Z",
     "start_time": "2020-05-20T20:14:21.561942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9007bd8b44834ff4b576971b06e03a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|     salesdb|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae94804ef1240a4b6335b14d4f62770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use salesdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5817c0f6790465c8eeb94c8ca55c620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| salesdb|          customer|      false|\n",
      "| salesdb|     customer_site|      false|\n",
      "| salesdb|     nyc_trips_csv|      false|\n",
      "| salesdb|           product|      false|\n",
      "| salesdb|  product_category|      false|\n",
      "| salesdb|       sales_order|      false|\n",
      "| salesdb|   sales_order_all|      false|\n",
      "| salesdb|sales_order_detail|      false|\n",
      "| salesdb|          supplier|      false|\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regular Spark SQL commands work great as we have enabled the feature 'Use Glue Data Catalog as the Hive metastore' for our AWS Glue Dev Endpoint by default. You can choose to run any spark-sql commands against these tables as an optional exercise \n",
    "\n",
    "You can click on the link to read more on [AWS Glue Data Catalog Support for Spark SQL Jobs](\n",
    "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-data-catalog-hive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above tables are pre-created for you using the AWS Glue crawler and you can see any new EMR cluster can seamlessly access the tables. Now, what about files that are in S3 (say CSV) which you need to use spark and create a table using a data frame and query it in sql? Use the section below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV files from S3 into spark program and create tables to query in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a746b0c0168441288d4ed7965f1654a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Boilerplate code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql import Row # A row of data in a DataFrame\n",
    "from pyspark.sql import GroupedData # Aggregation methods, returned by DataFrame.groupBy().\n",
    "from pyspark.sql import DataFrameNaFunctions # Methods for handling missing data (null values).\n",
    "from pyspark.sql import DataFrameStatFunctions # Methods for statistics functionality.\n",
    "from pyspark.sql import functions # List of built-in functions available for DataFrame.\n",
    "from pyspark.sql import types # List of data types available.\n",
    "from pyspark.sql import Window # For working with window functions.\n",
    "#End Boilerplate code\n",
    "\n",
    "nytrip_df = spark.read.csv(\"s3://glue-labs-001-180486424913/data/nyc_trips_csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7ecc5b0cc44fd08135483d0034a523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- trip_pickup_datetime: string (nullable = true)\n",
      " |-- trip_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- are_amt: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amt: string (nullable = true)\n",
      " |-- tolls_amt: string (nullable = true)\n",
      " |-- total_amt: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "#Print schema for the csv files read in previous step\n",
    "nytrip_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466f71ed74e54cfdb122d8336b9e4133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To run sql commands on the above spark dataframe, we will create a temp table. This table will persist through the life of this spark session\n",
    "nytrip_df.createOrReplaceTempView(\"temp_nytripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae68bbfac69a46179c9166aafadf6e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| salesdb|          customer|      false|\n",
      "| salesdb|     customer_site|      false|\n",
      "| salesdb|     nyc_trips_csv|      false|\n",
      "| salesdb|           product|      false|\n",
      "| salesdb|  product_category|      false|\n",
      "| salesdb|       sales_order|      false|\n",
      "| salesdb|   sales_order_all|      false|\n",
      "| salesdb|sales_order_detail|      false|\n",
      "| salesdb|          supplier|      false|\n",
      "|        |   temp_nytripdata|       true|\n",
      "+--------+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "#Validate your glue metastore and see if this table shows as a temp table as per third column.\n",
    "spark.sql(\"use salesdb\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform your data by denormalizing the tables and writing them in parquet format\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this activity, we will denormalize two tables and create a Parquet format output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad480467cc964523acb8cc09e099c1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------+---------------+-------------+------------+-------+---------+-------+-------+---------+---------+----+-----+\n",
      "|vendor_name|trip_pickup_datetime|trip_dropoff_datetime|passenger_count|trip_distance|payment_type|are_amt|surcharge|mta_tax|tip_amt|tolls_amt|total_amt|year|month|\n",
      "+-----------+--------------------+---------------------+---------------+-------------+------------+-------+---------+-------+-------+---------+---------+----+-----+\n",
      "|        VTS| 2010-03-12 21:37:00|  2010-03-12 21:51:00|              1|          4.5|         CSH|   12.9|      0.5|    0.5|    0.0|      0.0|     13.9|2010|   03|\n",
      "|        CMT| 2010-03-08 19:09:28|  2010-03-08 19:16:30|              1|          1.1|         Cas|    5.7|      1.0|    0.5|    0.0|      0.0|      7.2|2010|   03|\n",
      "|        VTS| 2010-03-20 22:53:00|  2010-03-20 23:10:00|              1|         2.19|         CSH|   10.1|      0.5|    0.5|    0.0|      0.0|     11.1|2010|   03|\n",
      "|        CMT| 2010-03-29 15:18:31|  2010-03-29 15:25:02|              1|          1.5|         Cas|    6.1|      0.0|    0.5|    0.0|      0.0|      6.6|2010|   03|\n",
      "|        VTS| 2010-03-12 21:48:00|  2010-03-12 21:52:00|              1|         0.52|         CSH|    4.1|      0.5|    0.5|    0.0|      0.0|      5.1|2010|   03|\n",
      "+-----------+--------------------+---------------------+---------------+-------------+------------+-------+---------+-------+-------+---------+---------+----+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Now fire at will using your standard ANSI sql queries against the tables in the catalog\n",
    "spark.sql(\"select * from temp_nytripdata\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the dataset\n",
    "\n",
    "Let's now denormalize the source tables where applicable and write out the data in Parquet format to the destination location. Note to change the S3 output_path in the cell below to appropriate bucket in your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d5c240b32d45b6b0efdc5e68fac1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- supplier_id: integer (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- quantity_per_unit: integer (nullable = true)\n",
      " |-- unit_price: decimal(10,2) (nullable = true)"
     ]
    }
   ],
   "source": [
    "adf=spark.sql(\"select * from product_category limit 5\")\n",
    "adf.printSchema()\n",
    "bdf=spark.sql(\"select * from product limit 5\")\n",
    "bdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:09.244279Z",
     "start_time": "2020-05-20T20:32:59.877787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ace23c445d54399b92c253f91ea9572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- supplier_id: integer (nullable = true)\n",
      "\n",
      "1000"
     ]
    }
   ],
   "source": [
    "# Create product denorm table joining between product_category and product tables\n",
    "product_df=spark.sql(\"SELECT a.category_id,a.category_name, b.product_id,b.name,b.supplier_id \\\n",
    "FROM product_category a, product b \\\n",
    "WHERE a.category_id=b.category_id\")\n",
    "product_df.printSchema()\n",
    "print(product_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=product_df.drop('category_id')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcadcd6b2e0e465eb6825b05d5f8bacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Write the entire product denorm table above into 1 file in parquet file format. Make Sure to update the S3 path below and replace it with your \n",
    "\n",
    "product_df.coalesce(1).write.mode(\"OVERWRITE\").parquet(\"s3://glue-labs-001-180486424913/data/sales_analytics/product_dim/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code for the AWS Glue Job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that your data is written to S3, switch to AWS console and validate you have 1 parquet file created at the above S3 location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step, Crawl the Transformed Data and create a table in your catalog for querying\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **sales_analytics_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/sales_analytics/**\n",
    "    - Page: Choose an IAM role\n",
    "        - IAM Role: Choose an existing IAM role **glue-labs-GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **sales_analytics**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the output data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:32.257757Z",
     "start_time": "2020-05-20T20:35:31.387873Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate your table exisits and start querying\n",
    "spark.sql(\"use sales_analytics\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!!! You have now successfully completed this exercise and learned how to use spark in your day-to-day\n",
    "[(Back to the top)](#top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
